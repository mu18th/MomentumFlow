{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ],
      "metadata": {
        "id": "degfgipj5pPR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2eqy24J4ylH"
      },
      "source": [
        "# **Brain MRI Tumor Segmentation** ðŸ§ \n",
        "\n",
        "In this exercise, you will apply what you learned in the previous lab to build a **brain tumor segmentation model** using a pretrained U-Net.\n",
        "\n",
        "## **Learning Objectives**\n",
        "- Build a custom Dataset class for MRI segmentation  \n",
        "- Use `segmentation_models_pytorch (SMP)` to load **U-Net** with a **pretrained encoder**  \n",
        "- Train and evaluate the model on brain MRI images\n",
        "\n",
        "## **Dataset**\n",
        "We will use the **LGG MRI Segmentation** dataset which contains:\n",
        "- **Brain MRI scans** (TIFF images)\n",
        "- **Tumor segmentation masks** (TIFF images)\n",
        "- Images are organized by patient folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m90FnIoh4ylI"
      },
      "source": [
        "## **Setup: Download Dataset and Install Dependencies**\n",
        "\n",
        "Run the following cells to download the dataset and install the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q segmentation_models_pytorch"
      ],
      "metadata": {
        "id": "tvotGyXk-6Zd",
        "outputId": "986932f5-dde3-4b41-9716-b693cbbd108e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPMPiEI14ylJ",
        "outputId": "66e4b4ee-ece5-4bcf-9b64-86fb91507c01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Download the LGG MRI Segmentation dataset\n",
        "path = kagglehub.dataset_download(\"mateuszbuda/lgg-mri-segmentation\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'lgg-mri-segmentation' dataset.\n",
            "Path to dataset files: /kaggle/input/lgg-mri-segmentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore the dataset structure**"
      ],
      "metadata": {
        "id": "V8uUG4LZ5PTX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8y8qdW_4ylJ",
        "outputId": "afa474f9-6ae5-4bd9-a166-ade522465e35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# List the contents of the dataset folder\n",
        "dataset_root = os.path.join(path, \"lgg-mri-segmentation\", \"kaggle_3m\")\n",
        "\n",
        "print(\"Dataset contents:\")\n",
        "print(os.listdir(dataset_root)[:10])  # Show first 10 patient folders"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset contents:\n",
            "['TCGA_DU_7010_19860307', 'TCGA_DU_8162_19961029', 'TCGA_FG_A4MT_20020212', 'TCGA_FG_5964_20010511', 'TCGA_DU_A5TS_19970726', 'TCGA_HT_7692_19960724', 'TCGA_DU_5849_19950405', 'TCGA_FG_A60K_20040224', 'TCGA_HT_7475_19970918', 'TCGA_FG_6691_20020405']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T-cyITH4ylJ"
      },
      "source": [
        "---\n",
        "\n",
        "## **Task 1: Explore the Dataset Structure**\n",
        "\n",
        "The dataset is organized by patient folders. Each folder contains:\n",
        "- MRI images (e.g., `TCGA_CS_4941_19960909_11.tif`)\n",
        "- Corresponding masks (e.g., `TCGA_CS_4941_19960909_11_mask.tif`)\n",
        "\n",
        "**Your task:** Write code to:\n",
        "1. Count the total number of image-mask pairs\n",
        "2. Store all image paths and mask paths in two lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObOxsyLb4ylJ"
      },
      "source": [
        "image_paths = []\n",
        "mask_paths = []\n",
        "\n",
        "# Loop through all patient folders\n",
        "for patient_folder in os.listdir(dataset_root):\n",
        "  patient_path = os.path.join(dataset_root, patient_folder)\n",
        "\n",
        "  if os.path.isdir(patient_path):\n",
        "    for filename in os.listdir(patient_path):\n",
        "      # TODO: Check if the file is a mask (contains \"_mask\")\n",
        "      # If it's a mask, add to mask_paths\n",
        "      # Otherwise, if it's a .tif file, add to image_paths\n",
        "\n",
        "      # YOUR CODE HERE\n",
        "      mask_paths.append(filename) if filename.contains(\"_mask\") else image_paths.append(filename)\n",
        "\n",
        "print(f\"Total images: {len(image_paths)}\")\n",
        "print(f\"Total masks: {len(mask_paths)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important:** The brain tumor dataset is **highly imbalanced**:\n",
        "- Many MRI slices have **no tumor at all** (empty masks)\n",
        "- Even in slices with tumors, the tumor region is typically only ~1-5% of the image\n",
        "\n",
        "If we train with all samples, the model learns to predict \"no tumor\" everywhere because that minimizes loss on the majority of pixels."
      ],
      "metadata": {
        "id": "hqLkWo5XjkXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select samples with tumors\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "samples_with_tumor = []\n",
        "\n",
        "for img_path, mask_path in zip(image_paths, mask_paths):\n",
        "  mask = np.array(Image.open(mask_path))\n",
        "  if np.sum(mask) > 0:  # Has tumor pixels\n",
        "    samples_with_tumor.append((img_path, mask_path))"
      ],
      "metadata": {
        "id": "hhDqjWcQjj_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = [s[0] for s in samples_with_tumor]\n",
        "mask_paths = [s[1] for s in samples_with_tumor]\n",
        "\n",
        "print(f\"Total samples: {len(samples_with_tumor)}\")"
      ],
      "metadata": {
        "id": "tMBhXwjRkWAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pmfzPTH4ylJ"
      },
      "source": [
        "---\n",
        "\n",
        "## **Task 2: Create the Dataset Class**\n",
        "\n",
        "Create a custom PyTorch Dataset class for the MRI segmentation task.\n",
        "\n",
        "**Requirements:**\n",
        "- Load images and their corresponding masks\n",
        "- Apply transforms to images (normalize with ImageNet stats)\n",
        "- Apply transforms to masks (resize, convert to tensor)\n",
        "- Return image-mask pairs"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rbdao64xvnj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmalMBk_4ylJ"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def remap_mask_binary(mask):\n",
        "  mask_np = mask.numpy().squeeze()\n",
        "  binary_mask = (mask_np != 0).astype(np.uint8)\n",
        "  return torch.from_numpy(binary_mask).unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snl52XXy4ylK"
      },
      "source": [
        "class BrainMRIDataset(Dataset):\n",
        "  def __init__(self, image_paths, mask_paths, transform=None, target_transform=None):\n",
        "    self.image_paths = image_paths\n",
        "    self.mask_paths = mask_paths\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    # TODO: Return the number of samples in the dataset\n",
        "    # YOUR CODE HERE\n",
        "    return None\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # TODO: Load the image and mask at index idx\n",
        "    # Hint: Use Image.open() and convert image to \"RGB\", mask to \"L\" (grayscale)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    image = None\n",
        "    mask = None\n",
        "\n",
        "    # Apply transforms\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    if self.target_transform:\n",
        "      mask = self.target_transform(mask)\n",
        "      mask = remap_mask_binary(mask)  # Convert to binary mask\n",
        "\n",
        "    return image, mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fgsQZFB4ylK"
      },
      "source": [
        "---\n",
        "\n",
        "## **Task 3: Define Transforms and Create DataLoaders**\n",
        "\n",
        "Define the image and mask transforms, then split the data and create DataLoaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOuLWomV4ylK"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TODO: Add transforms\n",
        "# Hint: ToTensor, Resize to (256, 256), Normalize with ImageNet mean/std\n",
        "# YOUR CODE HERE\n",
        "image_transforms = None\n",
        "\n",
        "# TODO: Add transforms\n",
        "# Hint: Resize to (256, 256) with NEAREST interpolation, PILToTensor\n",
        "# YOUR CODE HERE\n",
        "mask_transforms = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5XKufGq4ylK"
      },
      "source": [
        "# Split into train and test sets (80% train, 20% test)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "train_images, test_images, train_masks, test_masks = None\n",
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = BrainMRIDataset(train_images, train_masks, transform=image_transforms, target_transform=mask_transforms)\n",
        "\n",
        "test_dataset = BrainMRIDataset(test_images, test_masks, transform=image_transforms, target_transform=mask_transforms)\n",
        "\n",
        "# Create DataLoaders\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Nt_ZAa44ylK"
      },
      "source": [
        "---\n",
        "\n",
        "## **Task 4: Visualize Sample Data**\n",
        "\n",
        "Display some sample images with their corresponding masks to verify the data loading works correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPxeUWA14ylK"
      },
      "source": [
        "def denormalize(img):\n",
        "  mean = np.array([0.485, 0.456, 0.406])\n",
        "  std = np.array([0.229, 0.224, 0.225])\n",
        "  img = img.permute(1, 2, 0).numpy()  # CHW -> HWC\n",
        "  img = img * std + mean\n",
        "  img = np.clip(img, 0, 1)\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_pOTqxk4ylK"
      },
      "source": [
        "# Display 4 images with their masks side by side\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "for i in range(4):\n",
        "    # TODO: Get an image-mask pair from train_dataset\n",
        "    # Hint: Use train_dataset[i] to get the i-th sample\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    image, mask = None\n",
        "\n",
        "    # Display image (denormalize first)\n",
        "    axes[0, i].imshow(denormalize(image))\n",
        "    axes[0, i].set_title(f\"MRI Image {i+1}\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "\n",
        "    # Display mask\n",
        "    axes[1, i].imshow(mask.squeeze(), cmap=\"gray\")\n",
        "    axes[1, i].set_title(f\"Tumor Mask {i+1}\")\n",
        "    axes[1, i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCmyiOup4ylK"
      },
      "source": [
        "---\n",
        "\n",
        "## **Task 5: Create the U-Net Model**\n",
        "\n",
        "Use `segmentation_models_pytorch` to create a U-Net model with a pretrained encoder.\n",
        "\n",
        "**Requirements:**\n",
        "- Use a pretrained encoder backbone (e.g., ResNet34)\n",
        "- Configure for binary segmentation (1 output class)\n",
        "- Input: 3-channel RGB images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdQynpyn4ylL"
      },
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63T7JlTA4ylL"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "model = None\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2rowlxa4ylL"
      },
      "source": [
        "---\n",
        "\n",
        "## **Task 6: Define Training and Validation Functions**\n",
        "\n",
        "Implement the training and validation loops for the segmentation model.\n",
        "\n",
        "**Requirements:**\n",
        "- Training loop: forward pass, compute loss, backward pass, optimizer step\n",
        "- Validation loop: forward pass, compute loss (no gradients)\n",
        "- Use Binary Cross Entropy with Logits Loss (BCEWithLogitsLoss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPE0v8uy4ylL"
      },
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for images, masks in tqdm(dataloader):\n",
        "    # Move data to device\n",
        "    images, masks = images.to(device), masks.to(device).float()\n",
        "\n",
        "    # TODO: Complete the training step\n",
        "    # 1. Forward pass\n",
        "    # 2. Compute loss\n",
        "    # 3. Zero gradients\n",
        "    # 4. Backward pass\n",
        "    # 5. Update weights\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZagShXmq4ylL"
      },
      "source": [
        "# TODO: Task 6 (continued) - Complete the validation function\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, masks in dataloader:\n",
        "      images, masks = images.to(device), masks.to(device).float()\n",
        "\n",
        "      # TODO: Complete the validation step\n",
        "      # 1. Forward pass\n",
        "      # 2. Compute loss\n",
        "\n",
        "      # YOUR CODE HERE\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzsqmpkr4ylL"
      },
      "source": [
        "---\n",
        "\n",
        "## **Task 7: Train the Model**\n",
        "\n",
        "Set up the loss function, optimizer, and run the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoDX7hTM4ylL"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "# YOUR CODE HERE\n",
        "criterion = None\n",
        "optimizer = None\n",
        "\n",
        "num_epochs = 10  # Train for 10 epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zToHnvTG4ylL"
      },
      "source": [
        "# Run training\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "  val_loss = validate(model, test_loader, criterion, device)\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cQWpYD44ylL"
      },
      "source": [
        "### **Plot Training Loss Curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OotvoWBk4ylL"
      },
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\", marker='o')\n",
        "plt.plot(range(1, num_epochs+1), val_losses, label=\"Validation Loss\", marker='o')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QMWaAvY4ylL"
      },
      "source": [
        "---\n",
        "\n",
        "## **Task 8: Visualize Predictions**\n",
        "\n",
        "Compare the model's predictions against the ground truth masks on test images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYZLiTvU4ylL"
      },
      "source": [
        "# TODO: Task 8 - Visualize predictions on test images\n",
        "import random\n",
        "\n",
        "model.eval()\n",
        "\n",
        "fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
        "\n",
        "# Get random test samples\n",
        "indices = random.sample(range(len(test_dataset)), 4)\n",
        "\n",
        "for i, idx in enumerate(indices):\n",
        "  image, mask = test_dataset[idx]\n",
        "\n",
        "  # TODO: Get model prediction\n",
        "  # 1. Add batch dimension: image.unsqueeze(0)\n",
        "  # 2. Move to device\n",
        "  # 3. Get prediction: model(image)\n",
        "  # 4. Apply sigmoid to get probabilities\n",
        "  # 5. Threshold at 0.5 to get binary mask\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "  # Display results\n",
        "  axes[i, 0].imshow(denormalize(image))\n",
        "  axes[i, 0].set_title(\"MRI Image\")\n",
        "  axes[i, 0].axis(\"off\")\n",
        "\n",
        "  axes[i, 1].imshow(mask.squeeze(), cmap=\"gray\")\n",
        "  axes[i, 1].set_title(\"Ground Truth\")\n",
        "  axes[i, 1].axis(\"off\")\n",
        "\n",
        "  axes[i, 2].imshow(pred.squeeze(), cmap=\"gray\")\n",
        "  axes[i, 2].set_title(\"Prediction\")\n",
        "  axes[i, 2].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtlxFPzL4ylM"
      },
      "source": [
        "---\n",
        "\n",
        "### **Contributed by: Sattam Altwaim**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}